{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "TifgCpGLPTPF",
    "outputId": "c011ac0f-edb3-4195-e752-67162fa33c10"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-619e6c64eb0b>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-619e6c64eb0b>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    from GCN-model import *\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from dummy import *\n",
    "from GCN_model import *\n",
    "\n",
    "dtype = torch.long\n",
    "batch_size = 128\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 50\n",
    "n_nodes = 5\n",
    "hidden_dim = 10\n",
    "n_classes = 4\n",
    "dropout = 0.5\n",
    "'''\n",
    "def Normalize_Adj(A):\n",
    "    A_tilda = A + torch.eye(A.shape[1]).repeat(A.shape[0], 1, 1)\n",
    "    D_tilda = torch.diag_embed(torch.sum(A_tilda, 2).pow(-0.5))\n",
    "    A_hat = D_tilda.bmm(A_tilda).bmm(D_tilda)\n",
    "    return A_hat\n",
    "\n",
    "def get_dataset(n_train=65536, n_valid=8192, n_nodes=n_nodes, n_classes=n_classes):\n",
    "    # Generate random adjacency matrices\n",
    "    A = torch.randint(2, [n_train + n_valid, n_nodes, n_nodes])\n",
    "    A = A.to(dtype)\n",
    "    upper_tr = torch.triu(A, diagonal=1)\n",
    "    data =  upper_tr + torch.transpose(upper_tr, 1, 2)\n",
    "    data = Normalize_Adj(data) # Normalization\n",
    "    data = torch.split(data, split_size_or_sections=[n_train, n_valid], dim=0)\n",
    "    # Generating labels\n",
    "    train_y = torch.randint(n_classes, (n_train, n_nodes), dtype=dtype)\n",
    "    valid_y = torch.randint(n_classes, (n_valid, n_nodes), dtype=dtype)\n",
    "\n",
    "    train_data = TensorDataset(data[0], train_y)\n",
    "    valid_data = TensorDataset(data[1], valid_y)\n",
    "    return train_data, valid_data\n",
    "'''\n",
    "train_dataset, valid_dataset = get_dataset()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "X = torch.eye(n_nodes)\n",
    "\n",
    "                             ##############\n",
    "'''\n",
    "class GraphConvolutionLayer(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "    super().__init__()\n",
    "    self.fc = nn.Linear(input_dim, output_dim, bias=False)\n",
    "    torch.nn.init.xavier_uniform_(self.fc.weight)\n",
    "  def forward(self, X, A):\n",
    "    out = self.fc(X)\n",
    "    out = torch.bmm(A, out)  \n",
    "    return out\n",
    "\n",
    "class TwoLayerGCN(nn.Module):\n",
    "  def __init__(self, input_dim=n_nodes, hidden_dim=hidden_dim, n_classes=n_classes, dropout=dropout):\n",
    "    super().__init__()\n",
    "    self.gc1 = GraphConvolutionLayer(input_dim, hidden_dim)\n",
    "    self.gc2 = GraphConvolutionLayer(hidden_dim, n_classes)\n",
    "    self.dropout = dropout\n",
    "\n",
    "  def forward(self, X, A):\n",
    "    X = X.repeat(A.shape[0], 1, 1)\n",
    "    #print('X: ', X.shape)\n",
    "    out = self.gc1(X, A)\n",
    "    #print('Output after 1st GCN layer: ', out.shape)\n",
    "    out = F.relu(out)\n",
    "    out = F.dropout(out, self.dropout)\n",
    "    out = self.gc2(out, A)\n",
    "    #print('Output after 2nd GCN layer: ', out.shape)\n",
    "    #out = F.softmax(out) \n",
    "    return out\n",
    "'''\n",
    "model = TwoLayerGCN()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (A, labels) in enumerate(train_loader):\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X, A)\n",
    "        loss = criterion(outputs.transpose(1,2), labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 8 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss))\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for A, labels in valid_loader:\n",
    "        outputs = model(X, A)\n",
    "        _, predicted = torch.max(outputs, 2)\n",
    "        total += labels.numel()\n",
    "        correct += (predicted == labels).sum()\n",
    "\n",
    "    print('Accuracy of the network on the test data: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
